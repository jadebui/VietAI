{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II/ Tìm nghiệm bài toán bằng `TensorFlow`\n",
    "\n",
    "##### 1) (Full) batch gradient descent: đưa toàn bộ X và Y vào để train:\n",
    "\n",
    "Với cách 1, do đưa toàn bộ batch vào nên gradient ở mỗi vòng lặp ổn định. Cách này được khuyến khích sử dụng khi hàm cost của mình biết rõ là convex (không có nhiều hơn 1 điểm tối ưu cục bộ). Tuy nhiên, đối với những hàm phức tạp, thì cách 1 có thể ko bao giờ đạt tối ưu toàn cục được.\n",
    "\n",
    "##### 2) Stochastic gradient descent: đưa từng cặp (x, y) trong data X, Y vào để train :\n",
    "\n",
    "Đối với cách 2, do mình đưa vào từng cặp nên gradient ở mỗi vòng lặp sẽ rất nhiễu (noisy). Chính vì sự nhiễu này mà có trong qúa trình học, nó có thể giúp mô hình vượt qua được các điểm tối ưu cục bộ. Stochastic = random, thể hiện cho sự nhiễu.\n",
    "##### 3) Mini-batch gradient descent: bốc 1 lượng nhiều hơn 1 mẫu từ X, Y để train.\n",
    "\n",
    "Cách 3 là sự kết hợp giữa 1 và 2, cũng là cách dùng nhiều nhất trong deep learning. Trong các bài tới sẽ đề cập sau.\n",
    "\n",
    "Còn về bài tập thì thực ra hàm error của mình hoàn toàn convex nên dùng cách 1 hay 2 đều được. Nhưng cách 2 sẽ lâu hơn. Bạn có thể sửa code lại để kiểm tra thử."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Đưa dữ liệu vào"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_function import load_Boston_housing_data\n",
    "import numpy as np\n",
    "\n",
    "train_X, test_X, train_Y, test_Y = load_Boston_housing_data(feature_ind = [2,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nhập thư viện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Khai báo biến"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "training_epochs = 10000\n",
    "display_step = 1000\n",
    "n_samples, dimension = train_X.shape\n",
    "batch_size = n_samples # Full Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bài 6. Khai báo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement input and parameter for tensorflow.\n",
    "train_X = tf.constant(train_X, dtype=tf.float64)\n",
    "\n",
    "train_Y = tf.reshape(tensor=train_Y, shape=(-1, 1))\n",
    "train_Y = None # convert train_Y to tensor tf\n",
    "\n",
    "# Set model weights\n",
    "W = None # create weights variable to train. size=(dimension, 1)\n",
    "b = tf.Variable(np.random.normal(size=(1, 1)), trainable=True)\n",
    "print(W)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bài 7. Xây dựng mô hình hồi quy tuyến tính\n",
    "\n",
    "$$\\hat{y} = Xw + b $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: implement a linear regression function\n",
    "def tf_lr_hypothesis(X, W, b):\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bài 8. Viết hàm cost\n",
    "$$\\mathcal{E}(\\hat{y}, y) = \\frac{1}{2N}\\|\\bf{\\hat{y}} - \\bf{y}\\|^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: implement a cost function\n",
    "def tf_mse_cost(Y_hat, Y):\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bài 9. Viết hàm train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: implemement GD\n",
    "optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bài 10. Chạy chương trình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(training_epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        Y_hat = None # apply linear regression function here\n",
    "        mse_cost = None # apply mse cost here.\n",
    "    grads = tape.gradient(mse_cost, [W, b])\n",
    "    optimizer.apply_gradients(zip(grads, [W, b]))\n",
    "    if (epoch + 1) % display_step == 0:\n",
    "        print(\"Epoch:\", epoch + 1, \"| Cost:\", mse_cost.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
